{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BQKlUzk4KKR"
      },
      "source": [
        "Trying to use Just Pandas in this file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DHxR93zIb5b",
        "outputId": "1ef5c827-76af-4e4d-8df6-e30847b9ae19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "import datetime\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGZIL306XWlT"
      },
      "source": [
        "This Cell are for installation of Spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "# !pip install findspark\n",
        "!pyspark --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhVOu4LJcyhx",
        "outputId": "9d940ce5-85d3-43f1-8a61-1617cd4241e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824025 sha256=c6169c514c2ded6aca3ae2c856296e939c6391a955ab87d095e4537967455a30\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/59/a0/a1a0624b5e865fd389919c1a10f53aec9b12195d6747710baf\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n",
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.3.2\n",
            "      /_/\n",
            "                        \n",
            "Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.17\n",
            "Branch HEAD\n",
            "Compiled by user liangchi on 2023-02-10T19:57:40Z\n",
            "Revision 5103e00c4ce5fcc4264ca9c4df12295d42557af6\n",
            "Url https://github.com/apache/spark\n",
            "Type --help for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsnYizO6GehH"
      },
      "outputs": [],
      "source": [
        "# import findspark\n",
        "# findspark.init()\n",
        "# findspark.find()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "IrYq9RxF0pXU",
        "outputId": "02c54adc-09a0-4b44-853f-39a8c5eec2a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f2394be5ee0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://2b44b6116f9d:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>CCD</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext ,SparkConf\n",
        "# spark = SparkSession.builder\\\n",
        "#         .master(\"local\")\\\n",
        "#         .appName(\"Colab\")\\\n",
        "#         .config('spark.ui.port', '4050')\\\n",
        "#         .enableHiveSupport() \\\n",
        "#         .getOrCreate()\n",
        "\n",
        "# spark = SparkSession \\\n",
        "#     .builder \\\n",
        "#     .appName('CCDN') \\\n",
        "#     .master(\"local[*]\") \\\n",
        "#     .config(\"spark.driver.memory\", \"12g\") \\\n",
        "#     .getOrCreate()\n",
        "#print the SparkSession variable.\n",
        "# spark = SparkConf().setMaster(\"local[2]\").setAppName(\"CCDN\")\n",
        "# sc = SparkContext.getOrCreate();\n",
        "\n",
        "conf = pyspark.SparkConf().setAll([\n",
        "                                        ('spark.executor.memory', '8g'),\n",
        "                                        ('spark.driver.maxResultSize', '8g'),\n",
        "                                        ('spark.driver.memory','12g'),\n",
        "                                        ('spark.sql.execution.arrow.pyspark.enabled', 'true')\n",
        "                                        ])\n",
        "spark = SparkSession.builder.config(conf=conf).appName(\"CCD\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbq1dbUuYHgU"
      },
      "source": [
        "Start of working on Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-TsavIpdoU4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "474e91cf-2d83-466b-d1c8-8d09723d0c2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pyspark/pandas/__init__.py:49: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pyspark.pandas as ps\n",
        "ps.set_option('compute.ops_on_diff_frames', True)\n",
        "ps.set_option('compute.default_index_type', 'distributed-sequence')\n",
        "ps.set_option(\"display.max_rows\", 11)\n",
        "import math\n",
        "import sys\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s64ZeFR87SsE",
        "outputId": "25d96045-4d8d-4f45-962b-5196aeaad3e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.pandas.frame.DataFrame'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "userId    100000\n",
              "itemId    100000\n",
              "rating    100000\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# for tiny.data\n",
        "# df = spark.read.options(delimiter='\\t', header=False).csv(\"/content/drive/My Drive/Colab Notebooks/Datasets/tiny.data\")\n",
        "# df = df.toDF(\"userId\",\"itemId\",\"rating\",\"timestamp\")\n",
        "# df = df.drop(\"timestamp\")\n",
        "# df.printSchema()\n",
        "# df = df.to_pandas_on_spark()\n",
        "\n",
        "\n",
        "# for u.data\n",
        "df = ps.read_csv(\"/content/drive/My Drive/Colab Notebooks/Datasets/u.data\" , sep =\"\\t\" , names = [\"userId\", \"itemId\", \"rating\", \"timestamp\"])\n",
        "df = df.drop(\"timestamp\", axis=1)\n",
        "\n",
        "\n",
        "# df.printSchema()\n",
        "# df.show(6)\n",
        "print(type(df))\n",
        "# print(df)\n",
        "df.count()\n",
        "# print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbko6cfEdCzl",
        "outputId": "a5512562-3228-4d4d-8398-a5d8ff36ace2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.pandas.frame.DataFrame'>\n",
            "userId    int64\n",
            "itemId    int64\n",
            "rating    int64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Converting userId and itemId to integer\n",
        "df[['userId', 'itemId', 'rating']] = df[['userId', 'itemId', 'rating']].astype(int)\n",
        "print(type(df))\n",
        "print(df.dtypes)\n",
        "df = df.spark.local_checkpoint()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiJ2zza80Wkq"
      },
      "outputs": [],
      "source": [
        "def randomMatrix(dimension1, dimenstion2, rank):\n",
        "    return ps.DataFrame(abs(np.random.normal(scale=1./rank,size=(dimension1,dimenstion2))))\n",
        "\n",
        "def get_matrix_dimension(df):\n",
        "  # This function get max user)id and itam_id as reslult, V and H matrix dimension\n",
        "  userMax = df['userId'].max()\n",
        "  itemMax = df['itemId'].max()\n",
        "  userMax += 1\n",
        "  itemMax += 1\n",
        "  return userMax, itemMax\n",
        "\n",
        "def sub_V_WH(WH):\n",
        "  WH['R'] = WH['rating'] - WH['WH']\n",
        "  R = WH.drop(['rating', 'WH'], axis=1)\n",
        "  R = R.spark.local_checkpoint()\n",
        "  return R\n",
        "\n",
        "def mult_WH(W, H, WH, rank):\n",
        "  WH = WH.merge(W,on='userId').merge(H,on='itemId')\n",
        "  # print(WH)\n",
        "  colst = ''\n",
        "  colList = []\n",
        "  for k in range (rank):\n",
        "    colst += f\"WH['WC{k}'] * WH['HC{k}'] + \"\n",
        "    colList.append(f\"WC{k}\")\n",
        "    colList.append(f\"HC{k}\")\n",
        "  colst = colst[0:len(colst)-2]\n",
        "  # print(colst)\n",
        "  # print(colList)\n",
        "  WH['WH'] = eval(colst)\n",
        "  WH = WH.drop(colList, axis=1)\n",
        "  WH = WH.spark.local_checkpoint()\n",
        "  return WH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnfMKdFnETbs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e6aee28-b181-494d-94de-866fc825c2a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "944, 1683\n"
          ]
        }
      ],
      "source": [
        "# Split Dataframe to Train and Test\n",
        "num_split = 100000\n",
        "train_data = df.iloc[:num_split,:]\n",
        "test_data = df.iloc[num_split:,:]\n",
        "# print(train_data)\n",
        "# print(test_data)\n",
        "userMax, itemMax = get_matrix_dimension(train_data)\n",
        "print(f\"{userMax}, {itemMax}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrmBipSf9WRR"
      },
      "outputs": [],
      "source": [
        "def mult_Wk_Hk(W, H, WkHk, k):\n",
        "  # print(\"mult_Wk_Hk started...\")\n",
        "  # query = f\"select userId, itemId, rating, WC{k} * HC{k} as WH from WkHk, W, H where W.id = WkHk.userId and H.id = WkHk.itemId\"\n",
        "  WkHk = WkHk.merge(W[[f'WC{k}','userId']],on='userId').merge(H[[f'HC{k}','itemId']],on='itemId')\n",
        "  # print(WkHk)\n",
        "  WkHk['WkHk'] = WkHk[f'WC{k}'] * WkHk[f'HC{k}']\n",
        "  WkHk = WkHk.drop([f'WC{k}', f'HC{k}'], axis=1)\n",
        "  # print(WkHk)\n",
        "  # print(\"mult_Wk_Hk ended!\")\n",
        "  return WkHk\n",
        "\n",
        "def R_add_WkHk(R, WkHk):\n",
        "  # print(R)\n",
        "  # print(WkHk)\n",
        "  WkHk = WkHk.merge(R,on=['userId', 'itemId'])\n",
        "  WkHk['Rhat'] = WkHk['R'] + WkHk['WkHk']\n",
        "  Rhat = WkHk.drop(['rating', 'WkHk', 'R'], axis=1)\n",
        "  # print(Rhat)\n",
        "  Rhat = Rhat.spark.local_checkpoint()\n",
        "  return Rhat\n",
        "  # Rhat = spark1.sql(\"SELECT int(R.userId), int(R.itemId), (float(R.R) + float(WkHk.WH)) as Rhat  FROM R, WkHk where R.itemId = WkHk.itemId and R.userId = WkHk.userId\")\n",
        "\n",
        "def Rhat_minus_WkHk(Rhat, WkHk):\n",
        "  # print(\"Rhat_minus_WkHk started...\")\n",
        "  WkHk = WkHk.merge(Rhat,on=['userId', 'itemId'])\n",
        "  WkHk['R'] = WkHk['Rhat'] - WkHk['WkHk']\n",
        "  R = WkHk.drop(['rating', 'WkHk', 'Rhat'], axis=1)\n",
        "  # print(R)\n",
        "  R = R.spark.local_checkpoint()\n",
        "  # print(\"Rhat_minus_WkHk ended!\")\n",
        "  return R\n",
        "  # R = spark1.sql(\"SELECT int(Rhat.userId), int(Rhat.itemId), (float(Rhat.Rhat) - float(WkHk.WH)) as R FROM Rhat, WkHk where Rhat.itemId = WkHk.itemId and Rhat.userId = WkHk.userId\")\n",
        "\n",
        "def Update_W_H(W, H, Rhat, k, lamda):\n",
        "  # print(f\" k = {k}\")\n",
        "  # print(Rhat)\n",
        "  # print(\"Update_W_H started...\")\n",
        "  # print(\"Start Select aggregation...\")\n",
        "  # query = f\"SELECT Rhat.userId, sum(Rhat.Rhat * H.HC0) / (sum(power(H.HC0,2))+{lamda}) as value FROM Rhat, H where Rhat.itemId = H.id group by Rhat.userId\"\n",
        "\n",
        "  WkHk = Rhat.merge(H[[f'HC{k}','itemId']],on='itemId')\n",
        "  WkHk['s'] = WkHk['Rhat'] * WkHk[f'HC{k}']\n",
        "  WkHk['m'] = WkHk[f'HC{k}'] ** 2\n",
        "  # print(WkHk)\n",
        "  sums = WkHk.groupby('userId').agg({'s': 'sum'}).reset_index(level=0)\n",
        "  summ = WkHk.groupby('userId').agg({'m': 'sum'}).reset_index(level=0)\n",
        "  new_W_ik = sums.merge(summ,on='userId')\n",
        "  new_W_ik[f'WC{k}'] = new_W_ik['s'] / (new_W_ik['m'] + lamda)\n",
        "  new_W_ik = new_W_ik.drop(['s', 'm'], axis=1)\n",
        "  new_W_ik = new_W_ik.spark.local_checkpoint()\n",
        "\n",
        "  # query = f\"SELECT Rhat.itemId, sum(Rhat.Rhat * W.WC0) / (sum(power(W.WC0,2))+{lamda}) as value FROM W join Rhat on W.id = Rhat.userId group by Rhat.itemId\"\n",
        "  WkHk = Rhat.merge(W[[f'WC{k}','userId']],on='userId')\n",
        "  WkHk['s'] = WkHk['Rhat'] * WkHk[f'WC{k}']\n",
        "  WkHk['m'] = WkHk[f'WC{k}'] ** 2\n",
        "  # print(WkHk)\n",
        "  sums = WkHk.groupby('itemId').agg({'s': 'sum'}).reset_index(level=0)\n",
        "  summ = WkHk.groupby('itemId').agg({'m': 'sum'}).reset_index(level=0)\n",
        "  new_H_ik = sums.merge(summ,on='itemId')\n",
        "  new_H_ik[f'HC{k}'] = new_H_ik['s'] / (new_H_ik['m'] + lamda)\n",
        "  new_H_ik = new_H_ik.drop(['s', 'm'], axis=1)\n",
        "  new_H_ik = new_H_ik.spark.local_checkpoint()\n",
        "\n",
        "  # print(W)\n",
        "  W = W.drop(f'WC{k}', axis=1)\n",
        "  W = W.merge(new_W_ik, on = 'userId')\n",
        "  W = W.spark.local_checkpoint()\n",
        "  # print(new_W_ik)\n",
        "  # print(W)\n",
        "\n",
        "  # print(H)\n",
        "  H = H.drop(f'HC{k}', axis=1)\n",
        "  H = H.merge(new_H_ik, on = 'itemId')\n",
        "  H = H.spark.local_checkpoint()\n",
        "  # print(new_H_ik)\n",
        "  # print(H)\n",
        "  return W, H\n",
        "\n",
        "def ccd_iteration(W, H, R, k):\n",
        "  for rank in range(k):\n",
        "    WkHk = mult_Wk_Hk(W, H, V, rank)\n",
        "    # print(WkHk)\n",
        "    # print(\"WkHk calculated For Rhat.\")\n",
        "    Rhat = R_add_WkHk(R, WkHk)\n",
        "    # print(Rhat)\n",
        "    # print(\"Rhat calculated.\")\n",
        "    W, H = Update_W_H(W, H, Rhat, rank, lamda)\n",
        "\n",
        "    # print(W)\n",
        "    # print(H)\n",
        "    # sys.exit()\n",
        "    # print(\"W, H  Updated on k.\")\n",
        "    WkHk = mult_Wk_Hk(W, H, V, rank)\n",
        "    # WkHk.show()\n",
        "    # print(\"WkHk calculated for R.\")\n",
        "    R = Rhat_minus_WkHk(Rhat, WkHk)\n",
        "    # R.show()\n",
        "    # print(\"R calculated.\")\n",
        "  return W, H, R\n",
        "\n",
        "def rmse_WH_V(WH):\n",
        "  WH['diff'] = (WH['rating'] - WH['WH']) ** 2\n",
        "  rmse = math.sqrt(WH['diff'].mean())\n",
        "  return rmse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5J1Q8TazEPqv",
        "outputId": "761d08e6-6639-487c-f5a3-58817d6f1555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rank: \t 10 \n",
            "lamda: \t 60 \n",
            "\n",
            "944, 1683\n",
            "<class 'pyspark.pandas.frame.DataFrame'>\n",
            "RMSE: 3.644480046540599\n"
          ]
        }
      ],
      "source": [
        "# from numpy import genfromtxt\n",
        "k=10\n",
        "lamda=60\n",
        "print('rank: \\t',k,'')\n",
        "print('lamda: \\t',lamda,'\\n')\n",
        "\n",
        "print(f\"{userMax}, {itemMax}\")\n",
        "\n",
        "W = randomMatrix(userMax, k, k)\n",
        "H = randomMatrix(k, itemMax, k)\n",
        "H = H.T\n",
        "\n",
        "W = W.reset_index(level=0)\n",
        "H = H.reset_index(level=0)\n",
        "WcolDic = {\"index\": \"userId\"}\n",
        "HcolDic = {\"index\": \"itemId\"}\n",
        "for rank in range (k):\n",
        "  WcolDic[rank] = f\"WC{rank}\"\n",
        "  HcolDic[rank] = f\"HC{rank}\"\n",
        "# print(WcolDic)\n",
        "# print(HcolDic)\n",
        "W = W.rename(columns=WcolDic, errors=\"raise\")\n",
        "H = H.rename(columns=HcolDic, errors=\"raise\")\n",
        "\n",
        "tiny=False\n",
        "if tiny==True:\n",
        "  # Save to disk\n",
        "  # H.write.options(delimiter='\\t', header=True).csv(\"/content/drive/My Drive/Colab Notebooks/Datasets/Htiny.data\")\n",
        "  # W.write.options(delimiter='\\t', header=True).csv(\"/content/drive/My Drive/Colab Notebooks/Datasets/Wtiny.data\")\n",
        "  # Read from disk\n",
        "  W = ps.read_csv(\"/content/drive/My Drive/Colab Notebooks/Datasets/Wtiny.data\" , sep =\"\\t\")\n",
        "  H = ps.read_csv(\"/content/drive/My Drive/Colab Notebooks/Datasets/Htiny.data\" , sep =\"\\t\")\n",
        "  # H = spark.read.options(delimiter='\\t', header=True).csv(\"/content/drive/My Drive/Colab Notebooks/Datasets/Htiny.data\")\n",
        "  # W = spark.read.options(delimiter='\\t', header=True).csv(\"/content/drive/My Drive/Colab Notebooks/Datasets/Wtiny.data\")\n",
        "  # W = W.to_pandas_on_spark()\n",
        "  # H = H.to_pandas_on_spark()\n",
        "  W = W.rename(columns={\"id\": \"userId\", \"WC0\": \"WC0\", \"WC1\": \"WC1\"}, errors=\"raise\")\n",
        "  H = H.rename(columns={\"id\": \"itemId\", \"HC0\": \"HC0\", \"HC1\": \"HC1\"}, errors=\"raise\")\n",
        "  H = H.astype({'itemId':'int', 'HC0':'float', 'HC1':'float' })\n",
        "  W = W.astype({'userId':'int', 'WC0':'float', 'WC1':'float' })\n",
        "\n",
        "# print(W)\n",
        "# print(H)\n",
        "\n",
        "V = train_data\n",
        "V = V.spark.local_checkpoint()\n",
        "# print(V)\n",
        "\n",
        "WH = mult_WH(W, H, V, k)\n",
        "WH = WH.spark.local_checkpoint()\n",
        "# print(WH)\n",
        "print(type(WH))\n",
        "\n",
        "R = sub_V_WH(WH)\n",
        "R = R.spark.local_checkpoint()\n",
        "# print(R)\n",
        "\n",
        "# print(\"start calculation of rmse... \")\n",
        "rmse = rmse_WH_V(WH)\n",
        "print(f\"RMSE: {rmse}\")\n",
        "# print(\"calculation of rmse end... \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30W-xMu51Ela",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "989cfa9a-4354-4c16-f459-7ecf42e749f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0\n",
            "2023-02-19 16:08:46.916876\n",
            "RMSE: 2.2093256297135406\n",
            "Iteration: 1\n",
            "2023-02-19 16:10:19.643893\n",
            "RMSE: 1.2092355372254284\n",
            "Iteration: 2\n",
            "2023-02-19 16:11:23.859547\n",
            "RMSE: 1.2439692670873008\n",
            "Iteration: 3\n",
            "2023-02-19 16:12:20.720164\n",
            "RMSE: 1.2495702032807594\n",
            "Iteration: 4\n",
            "2023-02-19 16:13:15.517508\n",
            "RMSE: 1.2467468253619123\n",
            "Iteration: 5\n",
            "2023-02-19 16:14:05.001780\n",
            "RMSE: 1.247621310348523\n",
            "Iteration: 6\n",
            "2023-02-19 16:14:56.002108\n",
            "RMSE: 1.247382187651163\n",
            "Iteration: 7\n",
            "2023-02-19 16:15:45.670742\n",
            "RMSE: 1.2476432033313853\n",
            "Iteration: 8\n",
            "2023-02-19 16:16:34.248005\n",
            "RMSE: 1.2476815126907097\n",
            "Iteration: 9\n",
            "2023-02-19 16:17:23.388761\n",
            "RMSE: 1.2477036927664629\n"
          ]
        }
      ],
      "source": [
        "# print(R)\n",
        "# print(W)\n",
        "# print(H)\n",
        "# print(WH)\n",
        "iteration = 10\n",
        "for i in range(iteration):\n",
        "  print(f\"Iteration: {i}\")\n",
        "  print(datetime.datetime.now())\n",
        "\n",
        "  W, H, R = ccd_iteration(W, H, R, k)\n",
        "  # sys.exit()\n",
        "  # print(W)\n",
        "  # print(H)\n",
        "  # print(R)\n",
        "  if not (i+1)%1:\n",
        "    # print(\"start calculation of new WH for compute rmse... \")\n",
        "    WH = mult_WH(W, H, V, k)\n",
        "    # print(WH)\n",
        "    # print(\"calculation of new WH end... \")\n",
        "    # print(\"start calculation of rmse... \")\n",
        "    rmse = rmse_WH_V(WH)\n",
        "    print(f\"RMSE: {rmse}\")\n",
        "    # sys.exit()\n",
        "\n",
        "    # print(\"calculation of rmse end... \")\n",
        "    if rmse < 0.5:\n",
        "      print(\"Convergenced!\")\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "userMax, itemMax = get_matrix_dimension(WH)\n",
        "print(f\"{userMax}, {itemMax}\")\n",
        "# WH.count()\n",
        "print(WH)\n",
        "print(W)\n",
        "print(H)\n"
      ],
      "metadata": {
        "id": "vwPsFcc6Nc4Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ac24e25-a1d6-4f9f-bc96-b8bb7c8ad256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "944, 1683\n",
            "    userId  itemId  rating        WH      diff\n",
            "0       26     258       3  3.225774  0.050974\n",
            "1       26     930       2  1.738350  0.068461\n",
            "2       26    1015       3  0.883779  4.478389\n",
            "3       29     245       3  2.292197  0.500986\n",
            "4       26     125       4  2.675507  1.754283\n",
            "5       29     189       4  2.303345  2.878639\n",
            "6       26     685       3  2.379525  0.384989\n",
            "7       26     235       2  2.080135  0.006422\n",
            "8       29     332       4  2.420119  2.496023\n",
            "9       29     539       2  1.294670  0.497490\n",
            "10      26      25       3  2.620272  0.144193\n",
            "\n",
            "[Showing only the first 11 rows x 5 columns]\n",
            "    userId       WC0       WC1       WC2       WC3       WC4       WC5       WC6       WC7       WC8       WC9\n",
            "0      474  1.291672  0.995547  0.690028  0.624193  0.554745  0.449316  0.408958  0.423849  0.397280  0.310760\n",
            "1       26  0.798834  0.689502  0.492871  0.418925  0.349405  0.327160  0.307116  0.264381  0.220589  0.246800\n",
            "2       29  0.735918  0.619339  0.445678  0.381041  0.320749  0.299638  0.281047  0.246999  0.209935  0.228396\n",
            "3      541  1.127987  0.925409  0.639989  0.551886  0.478427  0.411714  0.381673  0.358655  0.315958  0.301615\n",
            "4       65  1.016065  0.826301  0.566320  0.483014  0.412486  0.359801  0.335875  0.307667  0.266508  0.264417\n",
            "5      191  0.644207  0.554267  0.407531  0.344905  0.288314  0.280626  0.265330  0.225611  0.187619  0.221021\n",
            "6      558  0.682764  0.563083  0.404428  0.348720  0.292255  0.267097  0.249125  0.224544  0.194759  0.198080\n",
            "7      418  0.422795  0.388562  0.291155  0.242483  0.198787  0.204418  0.195841  0.156067  0.123042  0.165504\n",
            "8      222  1.050984  0.902529  0.627624  0.547738  0.476634  0.423372  0.397855  0.358790  0.305297  0.316670\n",
            "9      293  1.060278  0.775535  0.535659  0.503851  0.452551  0.346560  0.307005  0.349017  0.345519  0.222141\n",
            "10     270  1.313283  1.042608  0.729583  0.642369  0.559650  0.474400  0.433766  0.426303  0.389020  0.342685\n",
            "\n",
            "[Showing only the first 11 rows x 11 columns]\n",
            "    itemId       HC0       HC1       HC2       HC3       HC4       HC5       HC6       HC7       HC8       HC9\n",
            "0      474  1.296725  1.007744  0.703479  0.622162  0.535428  0.450616  0.410791  0.407785  0.374264  0.318134\n",
            "1       26  0.871944  0.695233  0.488063  0.432849  0.377998  0.320333  0.294703  0.289330  0.262747  0.232554\n",
            "2       29  0.702988  0.602276  0.418186  0.355678  0.307924  0.272929  0.255583  0.230665  0.196537  0.206899\n",
            "3     1677  0.048689  0.037334  0.025692  0.023003  0.019980  0.016351  0.014788  0.015170  0.014203  0.011430\n",
            "4      964  0.354215  0.268218  0.192848  0.176295  0.158633  0.129589  0.117823  0.125257  0.119513  0.094683\n",
            "5      191  1.310699  1.040888  0.713821  0.616527  0.522503  0.450959  0.418755  0.392354  0.346691  0.323743\n",
            "6     1010  0.765058  0.604018  0.426831  0.381043  0.332507  0.281405  0.256499  0.256940  0.237027  0.203927\n",
            "7      418  1.003021  0.816280  0.557244  0.475898  0.409219  0.354772  0.331834  0.306112  0.265949  0.262652\n",
            "8       65  0.997845  0.772200  0.536013  0.467177  0.402472  0.340950  0.312947  0.307831  0.280685  0.247384\n",
            "9      541  0.660127  0.541131  0.375300  0.326896  0.285087  0.246070  0.227189  0.217062  0.192841  0.183396\n",
            "10     558  0.941641  0.722979  0.513887  0.462248  0.404300  0.337334  0.306000  0.313785  0.294052  0.242030\n",
            "\n",
            "[Showing only the first 11 rows x 11 columns]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}